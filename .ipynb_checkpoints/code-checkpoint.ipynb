{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import surfboard\n",
    "from surfboard.feature_extraction_multiprocessing import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Misc. adjustments\n",
    "pd.set_option('max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing files\n",
    "filelist = os.listdir(\"Source/vox\")\n",
    "\n",
    "#Read files into pandas\n",
    "source = pd.DataFrame(filelist)\n",
    "#Rename file name column\n",
    "source = source.rename(columns={0:'file'})\n",
    "#Filter out DS files from macOS\n",
    "source = source[source.file != \".DS_Store\"]\n",
    "#Reset index\n",
    "source = source.reset_index(drop = True)\n",
    "\n",
    "#Extraction of variables from filenames\n",
    "speaker_id = []\n",
    "condition = []\n",
    "index = []\n",
    "\n",
    "for i in range (0, len(source)):\n",
    "    id = source[\"file\"][i][0:2]\n",
    "    con = source[\"file\"][i][3]\n",
    "    indx = source[\"file\"][i][5:7]\n",
    "    \n",
    "    if id[0:1] == \"0\":\n",
    "        speaker_id.append(id[1])\n",
    "    else:\n",
    "        speaker_id.append(id)\n",
    "    \n",
    "    if indx[0:1] == \"0\":\n",
    "        index.append(indx[1])\n",
    "    else:\n",
    "        index.append(indx)\n",
    "    \n",
    "    condition.append(con)\n",
    "    \n",
    "#Adding variables to dataframe\n",
    "source[\"id\"] = speaker_id\n",
    "source[\"condition\"] = condition\n",
    "source[\"indx\"] = index\n",
    "\n",
    "source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create training & valuation dataframes\n",
    "def create_sets(subjects, n_train, n_val, conditions):\n",
    "    train_set = pd.DataFrame()\n",
    "    val_set = pd.DataFrame()\n",
    "    selection = subjects\n",
    "\n",
    "    for i in selection:\n",
    "        tmp_train = source[(source.id==str(i)) & (source.condition==conditions[0])]\n",
    "        tmp_train = tmp_train.sample(n_train)\n",
    "        train_set = train_set.append(tmp_train)\n",
    "        debug1 = tmp_train\n",
    "        ind = tmp_train[\"indx\"].tolist()\n",
    "        \n",
    "        tmp_val = source[(source.id==str(i)) & (source.condition==conditions[1])]\n",
    "        tmp_val2 = tmp_val[tmp_val[\"indx\"].isin(ind) == False]\n",
    "        debug2 = tmp_val2\n",
    "        val_set = val_set.append(tmp_val2.sample(n_val))\n",
    "\n",
    "\n",
    "    train_set = train_set.reset_index()\n",
    "    train_set.file = os.path.join(sys.path[0], \"Source/vox/\") + train_set.file \n",
    "    val_set = val_set.reset_index()\n",
    "    val_set.file = os.path.join(sys.path[0], \"Source/vox/\") + val_set.file \n",
    "\n",
    "    train_files = train_set.file.tolist()\n",
    "    val_files = val_set.file.tolist()\n",
    "    \n",
    "    return train_set, val_set, train_files, val_files, debug1, debug2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extraction function using surfboard\n",
    "def feature_extract(files):\n",
    "    features = surfboard.feature_extraction_multiprocessing.extract_features_from_paths(files, [\"mfcc\", 'f0_statistics', 'formants'], statistics_list=['mean'], sample_rate=44100, num_proc=3)\n",
    "    np.array(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing\n",
    "def processing(X_train, X_val, train_set, val_set):\n",
    "    y_train = np.array(train_set[\"id\"])\n",
    "    y_val = np.array(val_set[\"id\"])\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    lb = LabelEncoder()\n",
    "    y_train = to_categorical(lb.fit_transform(y_train))\n",
    "    y_val = to_categorical(lb.fit_transform(y_val))\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_val = ss.transform(X_val)\n",
    "    #X_test = ss.transform(X_test)\n",
    "    \n",
    "    return y_train, y_val, X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "def build_model(n_speakers):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    model = Sequential()\n",
    "    model.add(Dense(19, input_shape=(19,), activation = 'relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(6, activation = 'softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, mode='auto')\n",
    "    \n",
    "    return model, early_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Neural Network\n",
    "def train_model(model, early_stop, log=0, graph=0):\n",
    "    history = model.fit(X_train, y_train, batch_size=20, epochs=100,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[early_stop], verbose = log)\n",
    "    \n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    \n",
    "    if graph == 1:\n",
    "        # Check train accuracy and validation accuracy over epochs.\n",
    "        plt.figure(figsize=(12, 8))# Generate line plot of training, testing loss over epochs.\n",
    "        plt.plot(train_accuracy, label='Training Accuracy', color='#185fad')\n",
    "        plt.plot(val_accuracy, label='Validation Accuracy', color='orange')# Set title\n",
    "        plt.title('Training and Validation Accuracy by Epoch', fontsize = 25)\n",
    "        plt.xlabel('Epoch', fontsize = 18)\n",
    "        plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "        plt.xticks(range(0,100,5), range(0,100,5))\n",
    "        plt.legend(fontsize = 18);\n",
    "    \n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "selection=(1,3,5,8,10,11)\n",
    "n_train = 10\n",
    "n_val = 20-n_train\n",
    "conditions = [(\"0\",\"0\"),(\"0\",\"2\"),(\"2\",\"0\"),(\"2\",\"2\")]\n",
    "labels = [\"NV-NV\", \"NV-V\", \"V-NV\", \"V-V\"]\n",
    "iterations = 10\n",
    "n_speakers = len(selection)\n",
    "log = 0\n",
    "graph = 0\n",
    "\n",
    "results = []\n",
    "avg_results= []\n",
    "colours = [\"orange\", \"blue\", \"green\", \"red\"]\n",
    "\n",
    "for i in range(len(conditions)):\n",
    "    for j in range(iterations):\n",
    "        #Creating sets\n",
    "        [train_set, val_set, train_files, val_files, debug1, debug2] = create_sets(selection, n_train, n_val, conditions[i])\n",
    "        #print(debug1)\n",
    "        #print(debug2)\n",
    "        #Extract features\n",
    "        X_train = feature_extract(train_files)\n",
    "        X_val = feature_extract(val_files)\n",
    "\n",
    "        #Pre-Processing\n",
    "        [y_train, y_val, X_train, X_val] = processing(X_train, X_val, train_set, val_set)\n",
    "\n",
    "        #Build model\n",
    "        [model, early_stop] = build_model(n_speakers)\n",
    "\n",
    "        #Train\n",
    "        result = train_model(model, early_stop, log, graph)\n",
    "        results.append(result)\n",
    "\n",
    "    #Sum accuracy from all runs\n",
    "    sum_results = [0] * len(results[0])\n",
    "    for i in range (0, len(results)):\n",
    "        sum_results = np.add(sum_results, results[i])\n",
    "\n",
    "    #Take average of accuracy from all runs\n",
    "    avg_results.append([x / len(results) for x in sum_results])\n",
    "    #print(avg_results)\n",
    "\n",
    "# Check average accuracy\n",
    "plt.figure(figsize=(12, 8))# Generate line plot of training, testing loss over epochs.\n",
    "for i in range(len(conditions)):\n",
    "    plt.plot(avg_results[i], label=labels[i], color=colours[i])# Set title\n",
    "plt.title('Validation Accuracy by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(0,100,5), range(0,100,5))\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
