{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import surfboard\n",
    "import scipy.io.wavfile as wav\n",
    "from surfboard.feature_extraction_multiprocessing import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Misc. adjustments\n",
    "pd.set_option('max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing files\n",
    "filelist = os.listdir(\"Source/vox\")\n",
    "\n",
    "#Read files into pandas\n",
    "source = pd.DataFrame(filelist)\n",
    "#Rename file name column\n",
    "source = source.rename(columns={0:'file'})\n",
    "#Filter out DS files from macOS\n",
    "source = source[source.file != \".DS_Store\"]\n",
    "#Reset index\n",
    "source = source.reset_index(drop = True)\n",
    "\n",
    "#Extraction of variables from filenames\n",
    "speaker_id = []\n",
    "condition = []\n",
    "index = []\n",
    "duration=[]\n",
    "\n",
    "for i in range (0, len(source)):\n",
    "    id = source[\"file\"][i][0:2]\n",
    "    con = source[\"file\"][i][3]\n",
    "    indx = source[\"file\"][i][5:7]\n",
    "    (source_rate, source_sig) = wav.read(os.path.join(sys.path[0], \"Source/vox/\") + source[\"file\"][i])\n",
    "    dur = len(source_sig) / float(source_rate)\n",
    "\n",
    "    if id[0:1] == \"0\":\n",
    "        speaker_id.append(id[1])\n",
    "    else:\n",
    "        speaker_id.append(id)\n",
    "    \n",
    "    if indx[0:1] == \"0\":\n",
    "        index.append(indx[1])\n",
    "    else:\n",
    "        index.append(indx)\n",
    "    \n",
    "    condition.append(con)\n",
    "    duration.append(dur)\n",
    "    \n",
    "#Adding variables to dataframe\n",
    "source[\"id\"] = speaker_id\n",
    "source[\"condition\"] = condition\n",
    "source[\"indx\"] = index\n",
    "source[\"duration\"] = duration\n",
    "\n",
    "mean_duration = source[\"duration\"].mean()\n",
    "print(mean_duration)\n",
    "std_duration = source[\"duration\"].std()\n",
    "print(std_duration)\n",
    "\n",
    "source.file = os.path.join(sys.path[0], \"Source/vox/\") + source.file\n",
    "source_files = source[\"file\"].tolist()\n",
    "\n",
    "features = surfboard.feature_extraction_multiprocessing.extract_features_from_paths(source_files, [\"mfcc\", 'f0_statistics', 'formants'], statistics_list=['mean', \"std\"], sample_rate=44100, num_proc=3)\n",
    "source = pd.concat([source, features], axis=1, sort=False)\n",
    "\n",
    "source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create Training & Valuation Sets\n",
    "def create_sets(subjects, n_train, n_val, conditions):\n",
    "    train_set = pd.DataFrame()\n",
    "    val_set = pd.DataFrame()\n",
    "    selection = subjects\n",
    "    \n",
    "    if isinstance(conditions[0], tuple):\n",
    "        for i in selection:\n",
    "            tmp_train = source[(source.id==str(i)) & (source.condition==conditions[0][0])]\n",
    "            tmp_train2 = source[(source.id==str(i)) & (source.condition==conditions[0][1])]\n",
    "            tmp_train = tmp_train.sample(int(n_train/2))\n",
    "            ind = tmp_train[\"indx\"].tolist()\n",
    "            tmp_train2 = tmp_train2[tmp_train2[\"indx\"].isin(ind) == False]\n",
    "            tmp_train2 = tmp_train2.sample(int(n_train/2))\n",
    "            tmp_train = tmp_train.append(tmp_train2)\n",
    "            train_set = train_set.append(tmp_train)\n",
    "            ind = tmp_train[\"indx\"].tolist()\n",
    "                        \n",
    "            tmp_val = source[(source.id==str(i)) & (source.condition==conditions[1])]\n",
    "            tmp_val2 = tmp_val[tmp_val[\"indx\"].isin(ind) == False]\n",
    "            val_set = val_set.append(tmp_val2.sample(n_val))\n",
    "    else:\n",
    "        for i in selection:\n",
    "            tmp_train = source[(source.id==str(i)) & (source.condition==conditions[0])]\n",
    "            tmp_train = tmp_train.sample(n_train)\n",
    "            train_set = train_set.append(tmp_train)\n",
    "            ind = tmp_train[\"indx\"].tolist()\n",
    "            \n",
    "            if isinstance(conditions[1], tuple):\n",
    "                tmp_val = source[(source.id==str(i)) & (source.condition==conditions[1][0])]\n",
    "                tmp_val2 = source[(source.id==str(i)) & (source.condition==conditions[1][1])]\n",
    "                tmp_val = tmp_val.sample(int(n_val/2))\n",
    "                ind = tmp_val[\"indx\"].tolist()\n",
    "                tmp_val2 = tmp_val2[tmp_val2[\"indx\"].isin(ind) == False]\n",
    "                tmp_val2 = tmp_val2.sample(int(n_val/2))\n",
    "                tmp_val = tmp_val.append(tmp_val2)\n",
    "                val_set = val_set.append(tmp_val)\n",
    "                #print(tmp_val[\"indx\"])\n",
    "            else:\n",
    "                tmp_val = source[(source.id==str(i)) & (source.condition==conditions[1])]\n",
    "                tmp_val2 = tmp_val[tmp_val[\"indx\"].isin(ind) == False]\n",
    "                val_set = val_set.append(tmp_val2.sample(n_val))\n",
    "\n",
    "\n",
    "    train_set = train_set.reset_index()\n",
    "    val_set = val_set.reset_index()\n",
    "\n",
    "    X_train = train_set.drop(train_set.columns[[0, 1, 2, 3, 4, 5]], axis=1)\n",
    "    X_val = val_set.drop(val_set.columns[[0, 1, 2, 3, 4, 5]], axis=1)\n",
    "    \n",
    "    np.array(X_train)\n",
    "    np.array(X_val)\n",
    "    \n",
    "    return train_set, val_set, X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing\n",
    "def processing(X_train, X_val, train_set, val_set):\n",
    "    y_train = np.array(train_set[\"id\"])\n",
    "    y_val = np.array(val_set[\"id\"])\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    lb = LabelEncoder()\n",
    "    y_train = to_categorical(lb.fit_transform(y_train))\n",
    "    y_val = to_categorical(lb.fit_transform(y_val))\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_val = ss.transform(X_val)\n",
    "    \n",
    "    return y_train, y_val, X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Model\n",
    "def build_model(n_speakers):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    model = Sequential()\n",
    "    model.add(Dense(19, input_shape=((source.shape[1] - 5),), activation = 'relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_speakers, activation = 'softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, mode='auto')\n",
    "    \n",
    "    return model, early_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "def train_model(model, early_stop, log=0, graph=0):\n",
    "    history = model.fit(X_train, y_train, batch_size=10, epochs=75,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[early_stop], verbose = log)\n",
    "    \n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    \n",
    "    if graph == 1:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(train_accuracy, label='Training Accuracy', color='#185fad')\n",
    "        plt.plot(val_accuracy, label='Validation Accuracy', color='orange')\n",
    "        plt.title('Training and Validation Accuracy by Epoch', fontsize = 25)\n",
    "        plt.xlabel('Epoch', fontsize = 18)\n",
    "        plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "        plt.xticks(range(0,100,5), range(0,100,5))\n",
    "        plt.legend(fontsize = 18);\n",
    "    \n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "#Which subjects should be used in the experiment\n",
    "selection=(1,3,5,8,10,11,12,13,15,18)\n",
    "#Number of training samples per subject\n",
    "n_train = 10\n",
    "#Number of validation samples per subject (set to use all remianing samples after training samples have been chosen)\n",
    "n_val = 20-n_train\n",
    "#Conditions to be run (first element of tuple is training set, second element is validation set, tuple as element means set will be split between specfified conditions)\n",
    "#0 is non-variable, 2 is variable\n",
    "conditions = [(\"0\",\"0\"),(\"0\",\"2\"),(\"2\",\"2\"),(\"2\",\"0\"),((\"0\",\"2\"),\"0\"),((\"0\",\"2\"),\"2\"),(\"0\",(\"0\",\"2\")),(\"2\", (\"0\",\"2\"))]\n",
    "#Labels for conditions used for plotting and CSV export\n",
    "labels = [\"NV-NV\",\"NV-V\",\"V-V\",\"V-NV\",\"NV+V-NV\",\"NV+V-V\",\"NV-NV+V\",\"V-NV+V\"]\n",
    "#Number of iterations per condition (in other words, how many models will be trained per condition)\n",
    "iterations = 1\n",
    "#Number of speakers, set to default to length of selection list\n",
    "n_speakers = len(selection)\n",
    "#Option to activate keras logging while training\n",
    "log = 0\n",
    "#Option to enable graphs for each individual model (NOTE: The graphs will not appear until all models have been trained)\n",
    "graph = 0\n",
    "\n",
    "results2 = [ [] for _ in range(iterations) ]\n",
    "avg_results= []\n",
    "results_list = []\n",
    "colours = [\"orange\", \"blue\", \"green\", \"red\", \"purple\",\"dodgerblue\", \"yellow\", \"black\"]\n",
    "\n",
    "#Iterate through conditions\n",
    "for i in range(len(conditions)):\n",
    "    #Reset result list used for plotting average curves\n",
    "    results1 = []\n",
    "    \n",
    "    #Iterate through number of chosen iterations per condition\n",
    "    for j in range(iterations):\n",
    "        \n",
    "        #Creating sets\n",
    "        [train_set, val_set, X_train, X_val] = create_sets(selection, n_train, n_val, conditions[i])\n",
    "        \n",
    "        #Pre-Processing\n",
    "        [y_train, y_val, X_train, X_val] = processing(X_train, X_val, train_set, val_set)\n",
    "\n",
    "        #Build model\n",
    "        [model, early_stop] = build_model(n_speakers)\n",
    "\n",
    "        #Train\n",
    "        result = train_model(model, early_stop, log, graph)\n",
    "        results1.append(result)\n",
    "        result = result[74]\n",
    "        results2[j].append(result)\n",
    "\n",
    "        #Progress Counter\n",
    "        print(str(1+j+i*iterations) + \"/\" + str(iterations * len(conditions)))\n",
    "        \n",
    "    #Sum accuracy from all runs\n",
    "    sum_results = [0] * 75\n",
    "    for i in range (0, iterations):\n",
    "        sum_results = np.add(sum_results, results1[i])\n",
    "\n",
    "    #Take average of accuracy from all runs\n",
    "    avg_results.append([x / iterations for x in sum_results])\n",
    "\n",
    "#Plot validation curves of different models\n",
    "plt.figure(figsize=(12, 8))\n",
    "for k in range(len(conditions)):\n",
    "    plt.plot(avg_results[k], label=labels[k], color=colours[k])\n",
    "plt.title('Validation Accuracy by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Accuracy', fontsize = 18)\n",
    "plt.xticks(range(0,100,5), range(0,100,5))\n",
    "plt.legend(fontsize = 18);\n",
    "\n",
    "#Turn list into dataframe\n",
    "results_df = pd.DataFrame(results2, columns=labels)\n",
    "results_df.head()\n",
    "\n",
    "#Export dataframe as CSV file\n",
    "results_df.to_csv(r'/Users/jthiepler/Documents/GitHub/voice-classifier/results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
